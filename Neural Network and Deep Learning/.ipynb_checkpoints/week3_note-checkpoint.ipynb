{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hidden Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Neural Network Representation\n",
    "    * input layer/hidden layer(s)/ output layer\n",
    "2. Computation a Neural Network's output\n",
    "    * $z_i^{j} = {w_i^{j}}^Tx + b_i^{j}$\n",
    "    * $Z = W^TX + b, a = sigmoid(Z)$, 其中W是4*3，X是3*1，因此a, b, Z都是4*1\n",
    "    ![computation](figures/computation_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Vectorize across multiple examples\n",
    "    * make X, A, Z as a (n, m) matrix, n means the units of a layer, m means the number of samples.\n",
    "    * then the $z_{0, 0}$ means the means the 1st sample of first unit, which is $z_{1, 1} = w_1x_1+ b$, $z_{2, 1}$ means the 1st sample and 2nd unit, which is $z_{2, 1} = w_2x_1 + b$, while $z_{1, 2}$ means the 2nd sample and 1st unit, which is $z_{1, 2} = w_0x_1 + b$.\n",
    "    * here, both $w_i, x_i$ means a vector of weight or sample.\n",
    "4. Explaination of Vectorized implementation\n",
    "    * 对于一个样本 $WX_i = Z_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Activation Function\n",
    "    1. 几种常见的激活函数\n",
    "        * for hidden unit, tanh(x) is better than sigmoid(x), $tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "        * for output layer, we expect $\\hat{y}$ to be(0, 1), then sigmoid is better.\n",
    "        * activate fucntion can be different for different layer\n",
    "        * 如果z过大或过小，sigmoid和tanh梯度都很小，这时，修正线性单元ReLU可以弥补。\n",
    "    2. 如何选择激活函数\n",
    "        * 输出层可以用sigmoid，hidden layer可以用relu 或者leaky relu $a = max(0.01*z, z)$\n",
    "        * 使用relu要比tanh，sigmoid要快，因为没有梯度接近0的地方\n",
    "6. 为什么要使用激活函数\n",
    "    *  转化为非线性结构，否则 z=wx+b永远是线性的， 线性的隐层是没有用的。\n",
    "7. Derivatives of activation function\n",
    "    *  sigmoid: $g(z) = \\frac{1}{1 + e^{-z}} \\rightarrow \\frac{dg(z)}{dz} = \\frac{dg(z)}{d(1 + e^{-z})} \\cdot \\frac{d(1+e^{-z})}{dz} = \\frac{1}{1 + e^{-z}}(1 - \\frac{1}{1 + e^{-z}}) = g(z)(1 - g(z))$\n",
    "    * Others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Gradient Descent for neural networks\n",
    "    * Params: $w^1, b^1, w^2, b^2, n_x=[n^0, n^1, n^2=1]$, 其中$w^1->(n^0, n^1), w^2->(n^1, ^2)$\n",
    "    * Cost func: $J(w^1, b^1, w^2, b^2) = \\frac{1}{m}\\sum L(\\hat{y}, y)$\n",
    "    * Gradient descent{  \n",
    "        compute $\\hat{y_i}$  \n",
    "        w = w - dJ/dw  \n",
    "        b = b - dJ/db  \n",
    "        }\n",
    "    * Forward Propogation:  \n",
    "    $Z^1 = w^1X + b^1$  \n",
    "    $A^1 = relu(Z^1)$  \n",
    "    $Z^2 = w^2A^1 + b^2$  \n",
    "    $A^2 = \\sigma(Z^1)$ \n",
    "    * Backward Propogation:  \n",
    "    $dZ^2 = A^2 - Y$  \n",
    "    $dw^2 = \\frac{1}{m}dz^2{A^1}^T$  \n",
    "    $db^2 = \\frac{1}{m}np.sum(dz^2, axis=1, keepdims=True)$  \n",
    "    $dz^2 = {w^2}^Tdz^2 * g'(z^1)$*表示逐元素乘积  \n",
    "    $dw^1 = \\frac{1}{m}dz^1{X}^T$  \n",
    "    $db^1 = \\frac{1}{m}np.sum(dz^1, axis=1, keepdims=True)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Random initialization\n",
    "    * 初始化权重为接近于0的（不为0）的初始值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
