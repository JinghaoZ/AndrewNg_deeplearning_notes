{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression as a neural network\n",
    "1. Logistic Regression\n",
    "    1. Given x, want $\\hat{y} = p(y=1|x), 0 \\leq \\hat{y} \\leq1$.\n",
    "    2. output: $\\hat{y} = \\sigma(\\omega^Tx + b)$\n",
    "    3. $  z = \\omega^Tx + b, \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Loss Regression Cost Function\n",
    "    1. Given ${(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)},  \\hat{y_i}  \\approx y_i$\n",
    "    2. $L(\\hat{y}, y) = \\frac{1}{2}(\\hat{y} - y)^2$\n",
    "    3. 为了产生凸优化问题，定义交叉熵$L(\\hat{y}, y)=-(ylog(\\hat{y}) + (1-y)log(1-\\hat{y}))$\n",
    "    4. Cost function $J(w, b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}, y) = -\\frac{1}{m}\\sum_{i=1}^{m}[ylog(\\hat{y}) + (1-y)log(1-\\hat{y})]$\n",
    "    5. 即loss 是单一样本的指标，cost是整个训练集在参数w, b上的指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Gradient Descent\n",
    "    1. find w, b to minimize J(w, b)\n",
    "    2. Repeat {  \n",
    "        $ w := w - \\alpha \\frac{dJ(w)}{dw}$  \n",
    "        }\n",
    "    3. for multi-variable, {  \n",
    "        $w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}$  \n",
    "        $b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}$  \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Computation Graph\n",
    "    1. Set J(a, b, c) = 3(a + b*c)\n",
    "    2. foreward and backward propagation\n",
    "    ![foreward](figures/fp.png)\n",
    "    3. 链式法则，$\\frac{dJ}{dc} = \\frac{dJ}{dv}\\frac{dv}{du}\\frac{du}{dc}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Logistic Regression Gradient Descent\n",
    "    1. $(\\hat{y}, y)=-(ylog(\\hat{y}) + (1-y)log(1-\\hat{y}))$\n",
    "    ![lrgd](figures/lrgd.png)\n",
    "        * $\\frac{dL}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}$\n",
    "        * $\\frac{dL}{dz} = \\frac{dL}{da}\\frac{da}{dz}  = \\frac{dL}{da}  \\cdot a(1-a) = a-y$\n",
    "        * $\\frac{dL}{dw} = \\frac{dL}{da}\\frac{da}{dz}\\frac{dz}{dw} = xdz$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Batch Gradient Descent \n",
    "    1. 每次迭代更新全部w，最后除以样本数m\n",
    "    2. 使用向量运算迭代更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is vectorization\n",
    "    * $w \\in R^n, x \\in R^n$分别表示两个n维向量， $w^Tx +b$的两种实现\n",
    "    * 尽量避免for循环\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3, 4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250379.8787853438\n",
      "Vectorized version:33.2331657409668ms\n",
      "250379.8787853569\n",
      "For loop version:879.6980381011963ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "# create a 1000000-dims vector\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"Vectorized version:\" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i] * b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"For loop version:\" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Vectorizing Logistic Regression's Gradient Computation\n",
    "    * 由1.5.A可知 $dz_i = a_i - y_i$\n",
    "    * 令 $A = [a_1, a_2, ..., a_m], Y=[y_1, y_2, ..., y_m], dZ = [dz_1, dz_2, ..., dz_m] = A - Y$\n",
    "    * $db = \\sum_{i=1}^{m} dz_i$ = np.mean(dZ)\n",
    "    * $dw= \\frac{1}{m}[x_1, x_2, ..., x_m][dz_1; dz_2; ...; dz_m] = \\frac{1}{m}XdZ^T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w = np.random.rand(10)    \n",
    "b = np.zeros(10)  \n",
    "Z = np.dot(w.T, X) + b \n",
    "A = sigmoid(Z)  \n",
    "dZ = A - Y  \n",
    "dw = X * dZ.T / m  \n",
    "db = np.mean(dZ)  \n",
    "w = w - alpha * dw  \n",
    "b = b - alpha * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
