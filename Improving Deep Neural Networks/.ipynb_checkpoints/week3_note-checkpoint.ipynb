{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tuning process\n",
    "    1. 常见的超参数（重要程度递减）\n",
    "        * 学习率$\\alpha$\n",
    "        * 隐层数量layers\n",
    "        * mini-batch size\n",
    "        * 隐层单元数\n",
    "        * 学习率衰减率\n",
    "        * 滑动平均模型 $\\beta, \\beta_1, \\beta_2, \\varepsilon$\n",
    "     2. 超参数的取值\n",
    "         * 网格法 遍历所有可能的超参数组合 每次随机取一个测试\n",
    "         * 范围法 从某个表现较好的范围内 更加密集地取样\n",
    "2. use an appropriate scale to pick hyperparameters\n",
    "    1. 在对数坐标上取值\n",
    "        * 如果在(0,1)内线性取值，那么取道(0, 0.1)内的概率只有10%，对于学习率来说，大于0.1基本无效，所以大多数采样是无效的。\n",
    "        * 因此应该让更小的值具有更大的范围。令r$\\in$(-4, 0), $10^r$取值为(0.0001,1)而且(0.0001,0.001)和(0.01, 0.1)是等距的，这使他们采样概率也是相同的。\n",
    "3. Hyperparameter turning practice\n",
    "    1. retest hyperparameters occasionally\n",
    "    2. searching hyperparameters\n",
    "        * 当算力不足时，只关注一个模型\n",
    "        * 算力足够时，并行训练多个模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalizing activations in a network\n",
    "    1. implement batch norm in one layer:\n",
    "        * some intermediate value in NN like ($z^1, z^2, ..., z^m$)\n",
    "        * $\\mu = \\frac{1}{m}\\sum{z^i}$\n",
    "        * $\\sigma^2 = \\frac{1}{m}\\sum{(z^i - u)^2}$\n",
    "        * $z_{norm}^i = \\frac{z^i - u}{\\sigma^2 + \\varepsilon}$\n",
    "        * 如果不想总是归一化到均值0 方差1之内，可以添加参数$\\tilde{z}^i = \\gamma z_{norm}^i + \\beta$, 其中$\\gamma, \\beta$是可学习的参数\n",
    "2. Fit batch normalization in whole NN\n",
    "    1. 每一个神经元在前向传播时做两件事:\n",
    "        * 计算$z = wx + b$\n",
    "        * calculate $a = activate(z)$\n",
    "    2. Batch Normalization of z\n",
    "        * 得到z之后，首先对z进行归一化，得到$\\tilde{z}$\n",
    "        * 之后再计算a\n",
    "        * 参数从(w, b) 变为(w, b $\\beta, \\gamma$)\n",
    "    3. BN与mini-batch\n",
    "        * 每个batch使用自己的$\\mu, \\sigma$\n",
    "        * 使用mini-batch时，参数b会被消掉\n",
    "        * $\\beta, \\gamma$的shape等同与原来的b\n",
    "    4. BN只在隐藏层中使用，通过BP更新参数$w, \\beta, \\gamma$\n",
    "    5. mini-batch的BN有一定的正则化的作用，因为每次只使用一个batch的均值和方差，添加了一些噪音\n",
    "3. BN at test time\n",
    "   1. 单独估算$\\mu, \\sigma$\n",
    "       * 使用指数加权平均，由之前每个mini-batch的$\\mu, \\sigma$通过指数平均计算每一个隐层的均值和方差的期望值\n",
    "       * 使用以上参数和NN中训练得到的$\\beta, \\gamma$来计算$\\tilde{z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-classfication Softmax regression\n",
    "1. 多分类问题\n",
    "    * 最后一层隐层有n个神经元，表示n个不同类别\n",
    "    * softmax激活函数: $t = e^{z^l}, a^l = \\frac{e^{z^l}}{\\sum_{i=1}^{n}t_i}, z^l$表示最后一个隐层l层的z，a即为softmax激活后的输出\n",
    "2. Softmax\n",
    "    1. loss function\n",
    "        * $\\mathcal{L}(\\hat{y}, y) = -\\sum_{j=1}^ny_ilog(\\hat{y_i})$\n",
    "        * 由于y为1维向量，所以只有一个值为1， 其余为0， 例如$[0,1,0,0]^T$，因此每个样本的loss只会有一项，其余为0，即$-log(\\hat{y_2})$\n",
    "    2. cost\n",
    "        * $Y = [y_1, y_2, ...,y_m] \\in \\mathcal{R}^{n*m}, \\hat{Y} = [\\hat{y_1}, \\hat{y_2}, ..., \\hat{y_n}] \\in \\mathcal{R}^{n*m}$\n",
    "        * 首先由fp计算loss，然后bp计算梯度dz，$dz^l = \\hat{y}-y = \\frac{\\partial J}{\\partial z^l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning frameworks\n",
    "1. Tensorflow\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
