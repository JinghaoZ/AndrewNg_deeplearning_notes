{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mini-batch gradient descent\n",
    "    1. Batch vs. mini-batch\n",
    "        * Batch同时处理整个训练集样本\n",
    "        * Mini-batch 同时只处理mini-batch大小的样本，梯度的更新也已一个minibatch大小为基础，即每次计算J(w, b)时，m为mini-batch size.\n",
    "        * Mini-batch 更快\n",
    "    2. Mini-batch size\n",
    "        * size = m: 等同于BGD，每次迭代时间长\n",
    "        * size = 1: SGD, 每次选取一个样本迭代，因此有时会朝反方向迭代，会失去向量化带来的加速，效率底下。\n",
    "        * 如果样本集比较小 可以用BGD，如果使用mini-batch, 建议使用的size$2^6, 2^7, 2^8, 2^9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Exponentially weighted averages 指数加权平均 = moving average\n",
    "    1.  $v_t = \\beta v_{t-1} + (1 - \\beta)\\theta_t$ 表示最新状态的值也受之前的值的影响， 但是越远的日子影响越低($\\beta^t, 0<\\beta<1$)\n",
    "3. Bias correction for exponentially weighted averages\n",
    "    1. 在初始时期，指数平均将出现较大偏差，此时可以用$\\frac{v_t}{1 - \\beta^t}$代替原式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Gradient descent with momentum 动量梯度下降\n",
    "    1. 对于普通GD，如果学习率过大，那么会产生很大的抖动\n",
    "    2. 每次迭代我们期望在水平方向尽量快， 而在竖直方向尽量小 -> 动量梯度下降\n",
    "        * 计算dw db\n",
    "        * $V_{dw} = \\beta V_{dw} + (1-\\beta)dw$\n",
    "        * $V_{db} = \\beta V_{db} + (1-\\beta)db$\n",
    "        * $w = w - \\alpha V_{dw}, b = b - \\alpha V_{db}$\n",
    "    3. 减小梯度的摆动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. RMSprop\n",
    "    1. On iteration t:\n",
    "        * calculate dw, db on current mini-batch\n",
    "        * $S_{dw} = \\beta S_{dw} + (1 - \\beta)(dw)^2$\n",
    "        * $S_{db} = \\beta S_{db} + (1 - \\beta)(db)^2$\n",
    "        * update $w = w - \\alpha \\frac{dw}{\\sqrt{S_{dw}}}, b = b - \\alpha \\frac{db}{\\sqrt{S_{db}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Adam optimization algorithm(Adaptive Momentum estimation)\n",
    "    1. Adam将Momentum和RMSprop结合到一起，适用于大多NN\n",
    "    2. 梯度计算\n",
    "        * $V_{dw} = \\beta_1 V_{dw} + (1-\\beta_1)dw$, $V_{db} = \\beta_1 V_{db} + (1-\\beta_2)db$\n",
    "        * $S_{dw} = \\beta_2 S_{dw} + (1 - \\beta_2)(dw)^2$, $S_{db} = \\beta_2 S_{db} + (1 - \\beta_2)(db)^2$\n",
    "    3. 梯度修正\n",
    "        * $ V_{dw}^{c} = \\frac{V_{dw}}{1-\\beta^t}$, $ V_{db}^{c} = \\frac{V_{db}}{1-\\beta^t}$\n",
    "        * $ S_{dw}^{c} = \\frac{S_{dw}}{1-\\beta^t}$, $ S_{db}^{c} = \\frac{S_{db}}{1-\\beta^t}$\n",
    "        *  c means 'corrected'.\n",
    "    4. 梯度更新\n",
    "        * $w = w - \\alpha\\frac{V_{dw}^{c}}{\\sqrt{S_{dw}^{c}}+ \\varepsilon}$, $b = b - \\alpha\\frac{V_{db}^{corrrected}}{\\sqrt{S_{db}^{c}}+ \\varepsilon}$\n",
    "    5. 超参数\n",
    "        * 学习率$\\alpha$\n",
    "        * $\\beta_1, \\beta_2$， 建议$\\beta_1=0.9, \\beta_2=0.999$\n",
    "        * $\\varepsilon, 10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. learning rate decay 学习率衰减\n",
    "    1.  $\\alpha = \\frac{1}{1 + decay\\_rate * epoch\\_num} * \\alpha_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. local optimal\n",
    "    1. Saddle point鞍点 这个点的导数为0 但不是局部最优。鞍点的特征可能是凸函数或者凹函数 因此只要不是同时是凹函数 就不是局部最优\n",
    "    2. 平稳段是NN的主要问题，因为会减缓学习 导数接近0\n",
    "    3. Adam可以帮助快速走出平稳段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
