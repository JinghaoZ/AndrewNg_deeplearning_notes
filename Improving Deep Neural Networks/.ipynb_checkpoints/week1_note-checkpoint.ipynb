{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practival aspects of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train/Dev/Test sets\n",
    "    1. Data set:\n",
    "        * training set 60%  /  98%\n",
    "        * cross validation set 10%  / 1%\n",
    "        * test set  30%  / 1%\n",
    "        * 大数据时代，可以考虑将训练集比例增大\n",
    "    2. Mismatched train/test set\n",
    "        * 如果训练集和测试集数据来源不同，会造成误差，因此尽量保证训练集和测试集有相同分布\n",
    "        * 有时没有测试集， 只有验证集也是可以的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bias and variance\n",
    "    1. Bias\n",
    "        * High Bias = underfitting 欠拟合\n",
    "    2. Variance\n",
    "        * high variance = overfitting 过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Basic recipe for ML\n",
    "    1. high bias\n",
    "        * bigger network\n",
    "        * train longer\n",
    "    2. high variance\n",
    "        * more data\n",
    "        * regularization\n",
    "    3. training a bigger networks always no hurt\n",
    "4. bias-variance trade off in regularization\n",
    "    1. regularization in LR\n",
    "        * $\\min_{w, b}J(w, b) \\rightarrow \\frac{1}{m}\\sum_{i=1}{m}L(\\hat{y}^i, y^i) + \\frac{\\lambda}{2m}||w||^2_2$\n",
    "        * L2 regularization: $||w||^2_2 = \\sum_{j=1}^{n_x}w_j^2 = w^Tw$\n",
    "        * L1 regularization:  $||w|| = \\sum_{j=1}^{n_x}w_j$\n",
    "    2. regularization in neural network\n",
    "        * Frobenius regularization: $||w^{l}||^2_F = \\sum_{i=1}^{n^{l-1}}\\sum_{j=1}^{n^{l}}w_{ij^2}$\n",
    "        * regularization for bp:  \n",
    "        $dw^l = $(oridinal bp part)$ + \\frac{\\lambda}{m}w^l$  \n",
    "        $w^l = w^l - \\alpha dw^l = w^l - \\frac{\\alpha \\lambda}{m}w^l - \\alpha$(oridinal bp part)$ = (1 - \\frac{\\alpha \\lambda}{m})w^l - \\alpha$(oridinal bp part)\n",
    "5. regularization prevent overfitting\n",
    "    1. 以tanh为例，如果z=wx+b, 且z相对小，那么利用的就是tanh的近似线性部分呢，当$\\lambda$足够大时，正则化会使w变小来降低loss，此时整个nn就是近似线性回归，即在过拟合的部分会线性化，降低过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. dropout\n",
    "    1. 每个神经元有一定概率被drop\n",
    "    2. invert dropout: after dropout, divided a by keep_prob to keep the expectation of a\n",
    "    3. 在测试时不需要dropout， 因为a3的期望没有改变 因此不需要dropout\n",
    "7. Understanding dropout\n",
    "    1. 使nn不过度依赖某一个特征，因为任意一个特征都可能会被drop掉\n",
    "    2. choose keep_prob\n",
    "        * 参数最多的一层(w矩阵的参数最多)可以给予一个较低的keep_prob，剩下的层可以给一些更高的值或者不drop.\n",
    "        * 对于输入层，可以给一个keep_prob，但是不能太低(0.9-1.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.22358108  0.45986694 -1.30313942 -0.71691509  2.23786213]\n",
      " [-0.81889032 -0.50347893  0.47094644  0.45044138  0.5085071 ]\n",
      " [ 0.23755289  0.6483599   0.0488408   0.08587225  0.30850992]]\n",
      "[[False False False  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n",
      "[[-0.          0.         -0.         -0.89614386  2.79732766]\n",
      " [-1.0236129  -0.62934866  0.58868305  0.56305172  0.63563387]\n",
      " [ 0.29694112  0.81044988  0.061051    0.10734031  0.38563741]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "keep_prob = 0.8\n",
    "# suppose dropout with layer 3\n",
    "a3 = np.random.randn(15).reshape(3, 5)\n",
    "print(a3)\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob\n",
    "a3 = np.multiply(a3, d3)\n",
    "a3 /= keep_prob # invert drop out 确保a3期望不变\n",
    "print(d3)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Other regularization methods\n",
    "    1. data augmentation\n",
    "        * 通过翻转、剪切等方式增大数据量，生成假的训练数据\n",
    "    2. earily stopping\n",
    "        * 提前结束迭代，通过验证集误差辨别是否发生了过拟合\n",
    "        * earily stopping同时也结束了对cost function的优化，这是一个缺点\n",
    "    3. Orthogonalization(正交化)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Normalizing training sets\n",
    "    1. 将不同范围的向量，归一化到相似的范围内，可以使cost function更易于优化。\n",
    "    2. 一般不使特征处于不同数量级上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Vanishing/ exploding gradients\n",
    "    * 考虑激活函数为线性函数，则对l层nn， $\\hat{y} = w^lx$\n",
    "    1. 梯度消失\n",
    "        * 当$w^l$是对角大于1的对角阵时，$w^l -> w_{0,0}^l\\rightarrow \\infty$\n",
    "    2. 梯度爆炸\n",
    "        * 当$w^l$是对角小于1的对角阵时，$w^l -> w_{0,0}^l\\rightarrow 0$\n",
    "    3. 通过权重初始化可以抑制梯度消失和爆炸\n",
    "11. Weight initialization\n",
    "    1. Xavier 初始化 \n",
    "        * 设置权重的时候，添加一个上一层神经元特征数的倒数的平方根，即\n",
    "    $np.sqrt(\\frac{2}{n^{l-1}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Numerical approximation of gradients\n",
    "    1. Checking derivative computation\n",
    "        * $\\frac{f(\\theta + \\varepsilon) - f(\\theta - \\varepsilon)}{2\\varepsilon} \\approx g(\\theta)$\n",
    "    2. Gradient Checking\n",
    "        * 将所有W, b合并为一个向量$\\theta$，那么J(w, b)转为$J(\\theta)$\n",
    "        * $d\\theta_{approx}[i] = \\frac{J(\\theta_1, \\theta_2, ..., \\theta_i+\\varepsilon, ...) - J(\\theta_1, \\theta_2, ..., \\theta_i-\\varepsilon, ...)}{2\\varepsilon} \\approx d\\theta[i] = \\frac{\\partial J}{\\partial \\theta_i}$\n",
    "        * check whether $\\frac{||d\\theta_{a} - d\\theta||_2}{||d\\theta_{a}||_2  +  ||d\\theta||_2}$， 当$\\varepsilon < 10^{-7}$时，如果该项的值也约等于$10^{-7}$， 那么这个梯度的计算是正确的。\n",
    "    3. Gradient checking implmentation notes\n",
    "        * 只适用于debug阶段，train阶段不需要\n",
    "        * 如果梯度检验失败，那么需要检察所有components\n",
    "        * 需要正则化\n",
    "        * 对dropout无效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
